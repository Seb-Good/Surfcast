{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Surfcast.ca</h1>\n",
    "<h3>A Goodfellow Analytics Creation</h3>\n",
    "<h5>In partnership with Griffin Global</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Reset Notebook\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil import parser\n",
    "from dateutil import tz\n",
    "import scipy.interpolate\n",
    "import os\n",
    "from sqlalchemy import create_engine # database connection\n",
    "\n",
    "# Matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Hide ipython notebook warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 1</h1>\n",
    "<h3>Get HTML Database as List of Files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set URL path to NOAA 'gridded fields' database\n",
    "url = 'http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridded_fields/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set URL path to NOAA 'gridded fields' map files\n",
    "url_map = 'http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridded_fields/map_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File types of interest [wave, wind, surface current, surface temperature]\n",
    "#extension_list = ['wav', 'wnd', 'cur', 'swt', 'ice', 'o', 'e', 's', 'm', 'h']\n",
    "extension_list = ['wav', 'wnd', 'cur', 'swt', 'ice', 'o', 'e', 's', 'm', 'h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define NOAA database class\n",
    "class NoaaDB:\n",
    "    \n",
    "    \"\"\"\n",
    "    Class: NoaaDB\n",
    "        - This class converts the NCAST|FCAST FTP database list into a pandas DataFrame\n",
    "        \n",
    "        - The gridded fields filename format is:\n",
    "\n",
    "          LYYYYDDDHH.N.EXT\n",
    "\n",
    "          L    = lake letter (s=Superior, m=Michigan, h=Huron, e=Erie, o=Ontario)\n",
    "          YYYY = year at start of simulation (GMT)\n",
    "          DDD  = Day Of Year at start of simulation (GMT)\n",
    "          HH   = hr at start of simulation (GMT)\n",
    "          N    = Site Number\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize object\n",
    "    def __init__(self, url, extension_list):\n",
    "        \n",
    "        # Set object attributes\n",
    "        self.url = url                            # FTP 'gridded data' database URL\n",
    "        self.url_ncast = self.url  + 'NCAST/'     # FTP NCAST database URL  \n",
    "        self.url_fcast = self.url  + 'FCAST/'     # FTP FCAST database URL   \n",
    "        self.extension_list = extension_list      # List of file type extensions   \n",
    "        self.html_ncast = {}                      # HTML from FCAST page\n",
    "        self.html_fcast = {}                      # HTML from FCAST page\n",
    "        self.html_obj_ncast = {}                  # FCAST HTML Object\n",
    "        self.html_obj_fcast = {}                  # FCAST HTML Object\n",
    "        \n",
    "        # Current UTC time as GMT\n",
    "        self.current_datetime_GMT = datetime.utcnow().replace(tzinfo=tz.gettz('GMT'))\n",
    "        \n",
    "        # Database dataframe\n",
    "        self.df = pd.DataFrame(index=[], columns=['filename', 'file_extension', \n",
    "                                                  'filetype', 'lake', 'file_datetime', \n",
    "                                                  'current_datetime','forecast_type', 'file_url'])                 \n",
    "        \n",
    "    # Get NCAST database \n",
    "    def getNcast(self):\n",
    "        \n",
    "        # Get DataFrame row count\n",
    "        df_rows = self.df.shape[0]-1\n",
    "        \n",
    "        # Get HTML from database page\n",
    "        self.html_ncast   = requests.get(self.url_ncast)\n",
    "        \n",
    "        # Create BeautifulSoup object\n",
    "        self.html_obj_ncast   = BeautifulSoup(self.html_ncast.content)\n",
    "        \n",
    "        # Set database list as dataframe \n",
    "        for link in self.html_obj_ncast.findAll('a', href=True):\n",
    "            if (\n",
    "                link.contents[0].split('.')[-1] in self.extension_list and \n",
    "                link.contents[0][0] in self.extension_list\n",
    "               ):\n",
    "                \n",
    "                df_rows += 1  # row count                                                 \n",
    "                \n",
    "                filename = link.contents[0]                                         # file name\n",
    "                file_extension = link.contents[0].split('.')[-1]                    # file extension\n",
    "                \n",
    "                # file datetime as GMT\n",
    "                file_datetime = datetime.strptime(link.contents[0].split('.')[0]\n",
    "                                                  [1:len(link.contents[0].split('.')[0])], \"%Y%j%H\") # file datetime (GMT)\n",
    "                file_datetime = file_datetime.replace(tzinfo=tz.gettz('GMT'))\n",
    "                \n",
    "                # Set file type\n",
    "                if file_extension == 'wav':           # Wave\n",
    "                    filetype = 'WAVES'\n",
    "                elif file_extension == 'wnd':         # Wind\n",
    "                    filetype = 'WINDS'\n",
    "                elif file_extension == 'cur':         # Surface Current\n",
    "                    filetype = 'SURFACE CURRENTS'\n",
    "                elif file_extension == 'swt':         # Surface Temperature\n",
    "                    filetype = 'SURFACE TEMPS'\n",
    "                elif file_extension == 'ice':         # Ice Conditions\n",
    "                    filetype = 'ICE PARAMS'\n",
    "                \n",
    "                # Set great lake\n",
    "                if filename[0] == 'e':       # Lake Erie\n",
    "                    lake = 'erie'\n",
    "                elif filename[0] == 'h':     # Lake Huron\n",
    "                    lake = 'huron'\n",
    "                elif filename[0] == 'o':     # Lake Ontario\n",
    "                    lake = 'ontario'\n",
    "                elif filename[0] == 's':     # Lake Superior\n",
    "                    lake = 'superior'\n",
    "                elif filename[0] == 'm':     # Lake Michigan\n",
    "                    lake = 'michigan'            \n",
    "                \n",
    "                # save to dataframe\n",
    "                self.df.loc[df_rows] = [filename, file_extension, filetype, \n",
    "                                        lake, file_datetime, self.current_datetime_GMT, \n",
    "                                        'NCAST', self.url_ncast]                                   \n",
    "                          \n",
    "    # Get FCAST database \n",
    "    def getFcast(self):\n",
    "        \n",
    "        # Get DataFrame row count\n",
    "        df_rows = self.df.shape[0]-1\n",
    "        \n",
    "        # Get HTML from database page\n",
    "        self.html_fcast   = requests.get(self.url_fcast)\n",
    "        \n",
    "        # Create BeautifulSoup object\n",
    "        self.html_obj_fcast   = BeautifulSoup(self.html_fcast.content)\n",
    "        \n",
    "        # Set database list as dataframe \n",
    "        for link in self.html_obj_fcast.findAll('a', href=True):\n",
    "            if (\n",
    "                link.contents[0].split('.')[-1] in self.extension_list and \n",
    "                link.contents[0][0] in self.extension_list\n",
    "               ):\n",
    "                \n",
    "                df_rows += 1  # row count                                                 \n",
    "                \n",
    "                filename = link.contents[0]                                         # file name\n",
    "                file_extension = link.contents[0].split('.')[-1]                    # file extension\n",
    "                \n",
    "                # file datetime as GMT\n",
    "                file_datetime = datetime.strptime(link.contents[0].split('.')[0]\n",
    "                                                  [1:len(link.contents[0].split('.')[0])], \"%Y%j%H\")  # file datetime (GMT)\n",
    "                file_datetime = file_datetime.replace(tzinfo=tz.gettz('GMT'))\n",
    "                \n",
    "                # Set file type\n",
    "                if file_extension == 'wav':           # Wave\n",
    "                    filetype = 'WAVES'\n",
    "                elif file_extension == 'wnd':         # Wind\n",
    "                    filetype = 'WINDS'\n",
    "                elif file_extension == 'cur':         # Surface Current\n",
    "                    filetype = 'SURFACE CURRENTS'\n",
    "                elif file_extension == 'swt':         # Surface Temperature\n",
    "                    filetype = 'SURFACE TEMPS'\n",
    "                elif file_extension == 'ice':         # Ice Conditions\n",
    "                    filetype = 'ICE PARAMS'\n",
    "                \n",
    "                # Set great lake\n",
    "                if filename[0] == 'e':       # Lake Erie\n",
    "                    lake = 'erie'\n",
    "                elif filename[0] == 'h':     # Lake Huron\n",
    "                    lake = 'huron'\n",
    "                elif filename[0] == 'o':     # Lake Ontario\n",
    "                    lake = 'ontario'\n",
    "                elif filename[0] == 's':     # Lake Superior\n",
    "                    lake = 'superior'\n",
    "                elif filename[0] == 'm':     # Lake Michigan\n",
    "                    lake = 'michigan'       \n",
    "                \n",
    "                # save to dataframe\n",
    "                self.df.loc[df_rows] = [filename, file_extension, filetype, \n",
    "                                        lake, file_datetime, self.current_datetime_GMT, \n",
    "                                        'FCAST', self.url_fcast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create NoaaDB object\n",
    "noaa_files = NoaaDB(url, extension_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get NCAST files \n",
    "noaa_files.getNcast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get FCAST files \n",
    "noaa_files.getFcast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>filetype</th>\n",
       "      <th>lake</th>\n",
       "      <th>file_datetime</th>\n",
       "      <th>current_datetime</th>\n",
       "      <th>forecast_type</th>\n",
       "      <th>file_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e201611706.0.ice</td>\n",
       "      <td>ice</td>\n",
       "      <td>ICE PARAMS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-04-26 06:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e201611712.0.ice</td>\n",
       "      <td>ice</td>\n",
       "      <td>ICE PARAMS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-04-26 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e201611718.0.ice</td>\n",
       "      <td>ice</td>\n",
       "      <td>ICE PARAMS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-04-26 18:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e201611800.0.ice</td>\n",
       "      <td>ice</td>\n",
       "      <td>ICE PARAMS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-04-27 00:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e201611806.0.ice</td>\n",
       "      <td>ice</td>\n",
       "      <td>ICE PARAMS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-04-27 06:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename file_extension    filetype  lake       file_datetime  \\\n",
       "0  e201611706.0.ice            ice  ICE PARAMS  erie 2016-04-26 06:00:00   \n",
       "1  e201611712.0.ice            ice  ICE PARAMS  erie 2016-04-26 12:00:00   \n",
       "2  e201611718.0.ice            ice  ICE PARAMS  erie 2016-04-26 18:00:00   \n",
       "3  e201611800.0.ice            ice  ICE PARAMS  erie 2016-04-27 00:00:00   \n",
       "4  e201611806.0.ice            ice  ICE PARAMS  erie 2016-04-27 06:00:00   \n",
       "\n",
       "            current_datetime forecast_type  \\\n",
       "0 2016-06-08 00:54:35.509431         NCAST   \n",
       "1 2016-06-08 00:54:35.509431         NCAST   \n",
       "2 2016-06-08 00:54:35.509431         NCAST   \n",
       "3 2016-06-08 00:54:35.509431         NCAST   \n",
       "4 2016-06-08 00:54:35.509431         NCAST   \n",
       "\n",
       "                                            file_url  \n",
       "0  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "2  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "3  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "4  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Dataframe of NoaaDB files \n",
    "noaa_files.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>filetype</th>\n",
       "      <th>lake</th>\n",
       "      <th>file_datetime</th>\n",
       "      <th>current_datetime</th>\n",
       "      <th>forecast_type</th>\n",
       "      <th>file_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>s201615900.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-07 00:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>s201615912.0.cur</td>\n",
       "      <td>cur</td>\n",
       "      <td>SURFACE CURRENTS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>s201615912.0.swt</td>\n",
       "      <td>swt</td>\n",
       "      <td>SURFACE TEMPS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>s201615912.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>s201615912.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename file_extension          filetype      lake  \\\n",
       "1107  s201615900.0.wnd            wnd             WINDS  superior   \n",
       "1108  s201615912.0.cur            cur  SURFACE CURRENTS  superior   \n",
       "1109  s201615912.0.swt            swt     SURFACE TEMPS  superior   \n",
       "1110  s201615912.0.wav            wav             WAVES  superior   \n",
       "1111  s201615912.0.wnd            wnd             WINDS  superior   \n",
       "\n",
       "           file_datetime           current_datetime forecast_type  \\\n",
       "1107 2016-06-07 00:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "1108 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "1109 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "1110 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "1111 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "\n",
       "                                               file_url  \n",
       "1107  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1108  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1109  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1110  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1111  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Dataframe of NoaaDB files \n",
    "noaa_files.df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 2</h1>\n",
    "<h3>Find Most Recently Uploaded Files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NoaaData():\n",
    "    \n",
    "    \"\"\"\n",
    "    NOAA database file class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, noaa_files, url_map):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize object\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set object attributes\n",
    "        self.noaa_files = noaa_files              # user input NoaaDB object (Pandas Dataframe of all files in Noaa DB)\n",
    "        self.df_ncast = {}                        # downloaded NCAST text files as DataFrame\n",
    "        self.df_fcast = {}                        # downloaded FCAST text files as DataFrame\n",
    "        self.df = {}                              # conbined NCAST and FCAST DataFrames of most recent 120 hr forecast\n",
    "        self.url_map = url_map                    # url containing map files\n",
    "        \n",
    "        # Set NOAA File attributes\n",
    "        self.attributes = {\n",
    "            'wave':        ['grid_number', 'wave_height', 'wave_direction', 'wave_period'],  \n",
    "            'wind':        ['grid_number', 'wind_speed', 'wind_direction'],\n",
    "            'temperature': ['grid_number', 'surface_temperature'],\n",
    "            'current':     ['grid_number', 'current_speed', 'current_direction'],\n",
    "            'ice':         ['grid_number', 'ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']\n",
    "        }\n",
    "\n",
    "        # Set map path\n",
    "        self.map_path = r'C:\\Users\\Sebastian\\Projects\\Websites\\Surfcast\\GetData\\GridFiles'\n",
    "        \n",
    "        # Get Newest NCAST Files in database and save as DataFrame\n",
    "        maxtime = noaa_files.df[(noaa_files.df.forecast_type == 'NCAST')]['file_datetime'].max()\n",
    "        self.newest_files_ncast = noaa_files.df[(noaa_files.df.forecast_type == 'NCAST') &\n",
    "                                                (noaa_files.df.file_datetime == maxtime)].reset_index(drop=True)\n",
    "        \n",
    "        # Get Newest FCAST Files in database and save as DataFrame\n",
    "        maxtime = noaa_files.df[(noaa_files.df.forecast_type == 'FCAST')]['file_datetime'].max()\n",
    "        self.newest_files_fcast = noaa_files.df[(noaa_files.df.forecast_type == 'FCAST') & \n",
    "                                                (noaa_files.df.file_datetime == maxtime)].reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def df_setup(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Set up empty DataFrames to hold text file data\n",
    "        \"\"\"\n",
    " \n",
    "        # Get list of unique NCAST file types and lakes\n",
    "        filetype_ncast = self.newest_files_ncast.filetype.unique()\n",
    "        \n",
    "        # Setup NCAST DataFrame\n",
    "        self.df_ncast  = pd.DataFrame(columns=['year', 'day', 'hour', 'datetime',                  \n",
    "                                               'grid_number', 'latitude', 'longitude', \n",
    "                                               'map', 'lake'])   \n",
    "        \n",
    "        if any('WAVES' in s for s in filetype_ncast):                         # wave\n",
    "            for col in self.attributes['wave']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index) \n",
    "                \n",
    "        if any('WINDS' in s for s in filetype_ncast):                         # wind              \n",
    "            for col in self.attributes['wind']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index)\n",
    "                \n",
    "        if any('SURFACE TEMPS' in s for s in filetype_ncast):                 # temperature\n",
    "            for col in self.attributes['temperature']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index)\n",
    "                \n",
    "        if any('SURFACE CURRENTS' in s for s in filetype_ncast):              # current\n",
    "            for col in self.attributes['current']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index)\n",
    "                \n",
    "        if any('ICE PARAMS' in s for s in filetype_ncast):                    # ice\n",
    "            for col in self.attributes['ice']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index)\n",
    "\n",
    "        # Get list of unique FCAST file types and lakes\n",
    "        filetype_fcast = self.newest_files_fcast.filetype.unique()\n",
    "\n",
    "        # Setup FCAST DataFrame\n",
    "        self.df_fcast  = pd.DataFrame(columns=['year', 'day', 'hour', 'datetime',              \n",
    "                                               'grid_number', 'latitude', 'longitude', \n",
    "                                               'map', 'lake'])   \n",
    "        \n",
    "        if any('WAVES' in s for s in filetype_fcast):                         # wave\n",
    "            for col in self.attributes['wave']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index) \n",
    "                \n",
    "        if any('WINDS' in s for s in filetype_fcast):                         # wind              \n",
    "            for col in self.attributes['wind']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index)\n",
    "                \n",
    "        if any('SURFACE TEMPS' in s for s in filetype_fcast):                 # temperature\n",
    "            for col in self.attributes['temperature']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index)\n",
    "                \n",
    "        if any('SURFACE CURRENTS' in s for s in filetype_fcast):              # current\n",
    "            for col in self.attributes['current']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index)\n",
    "                \n",
    "        if any('ICE PARAMS' in s for s in filetype_fcast):                    # ice\n",
    "            for col in self.attributes['ice']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index)\n",
    "\n",
    "                \n",
    "   \n",
    "\n",
    "    @classmethod\n",
    "    def getHeaderValue(cls, val):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to get and update text file header column\n",
    "        \"\"\"\n",
    "\n",
    "        global headerString\n",
    "\n",
    "        if 'dat' in val and val != headerString:\n",
    "            headerString = val\n",
    "            return headerString\n",
    "        else:\n",
    "            return headerString\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def getMapFile(cls, val, lake):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to format map string\n",
    "        \"\"\"\n",
    "\n",
    "        map = val.split('/')[-1]\n",
    "        map = map.split('.')[0] + '.' + 'map'\n",
    "\n",
    "        if lake == 'superior':\n",
    "            map = 'superior' + map.split('sup')[1]\n",
    "\n",
    "        return map\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def getAttributeList(cls, fileType, attributes):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to get list of attributes based on filetype\n",
    "        \"\"\"\n",
    "\n",
    "        if  fileType == 'WAVES':\n",
    "            return attributes['wave']\n",
    "        elif fileType == 'WINDS':\n",
    "            return attributes['wind']\n",
    "        elif fileType == 'SURFACE TEMPS':\n",
    "            return attributes['temperature']\n",
    "        elif fileType == 'SURFACE CURRENTS':\n",
    "            return attributes['current']\n",
    "        elif fileType == 'ICE PARAMS':\n",
    "            return attributes['ice']\n",
    "        \n",
    "        \n",
    "                \n",
    "                        \n",
    "    def df_fill(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Fill NCAST and FCAST DataFrames with text file data\n",
    "        \"\"\"\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        # NCAST \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "        # Loop through lakes\n",
    "        for lake in self.newest_files_ncast.lake.unique():\n",
    "\n",
    "            # Get DataFrame of attribute files to downhole\n",
    "            df_lake = self.newest_files_ncast[(self.newest_files_ncast.lake == lake)]\n",
    "            \n",
    "            # Set attribute counter\n",
    "            attributeCount = 0\n",
    "            \n",
    "            # Loop through atributes\n",
    "            for df_index in df_lake.index:\n",
    "                \n",
    "                # Update attribute counter\n",
    "                attributeCount += 1\n",
    "                \n",
    "                # Get list of attributes based on filetype\n",
    "                cols = self.getAttributeList(self.newest_files_ncast.filetype[df_index], self.attributes)\n",
    "\n",
    "                # Download lake attribute text file and save as DataFrame\n",
    "                dfFile = pd.read_table(\n",
    "                    self.newest_files_ncast.file_url[df_index] + self.newest_files_ncast.filename[df_index], \n",
    "                    header=None,\n",
    "                    names=['data']\n",
    "                )\n",
    "\n",
    "                # Set global variable text file header\n",
    "                global headerString\n",
    "                headerString = dfFile.ix[0, 'data']\n",
    "       \n",
    "                # Get text file header as new DataFrame column   \n",
    "                dfFile['header'] = dfFile['data'].map(lambda x: self.getHeaderValue(x))\n",
    "\n",
    "                # Add Lake column\n",
    "                dfFile['lake'] = lake\n",
    "                \n",
    "                # Add CAST Type\n",
    "                dfFile['forecast_type'] = 'NCAST'\n",
    "\n",
    "                # Remove header rows\n",
    "                dfFile['data'] = dfFile['data'].map(lambda x: np.nan if 'dat' in x else x)\n",
    "                dfFile = dfFile.dropna()\n",
    "\n",
    "                # Extract date and map information from header and set as DataFrame columns\n",
    "                dfFile[['year', 'day', 'hour', 'map']] = dfFile['header'].str.split(return_type='frame').ix[:, 0:3]\n",
    "\n",
    "                # Parse attribute column as set as DataFrame columns\n",
    "                dfFile[cols] = dfFile['data'].str.split(return_type='frame')\n",
    "\n",
    "                # Formate map string column\n",
    "                dfFile['map'] = dfFile['map'].map(lambda x: self.getMapFile(x, lake))\n",
    "                \n",
    "                # Add Datetime Object\n",
    "                dfFile['datetime'] = dfFile.apply(lambda x: \n",
    "                                                  datetime.strptime(x['year'] + x['day'] + x['hour'], \"%Y%j%H\"), \n",
    "                                                  axis=1)\n",
    "\n",
    "                # Drop useless columns \n",
    "                dfFile = dfFile.drop('data', axis=1).drop('header', axis=1)\n",
    "                \n",
    "                # Merge lake specific attribute DataFrames\n",
    "                if attributeCount == 1:\n",
    "                    dfLake = dfFile\n",
    "                else:\n",
    "                    dfLake = pd.merge(dfLake, dfFile[cols + ['datetime']])\n",
    "                \n",
    "            # Get map grid locations\n",
    "            mapFile = pd.read_table(self.url_map + dfFile.ix[dfFile.index[0], 'map'], header=None, \n",
    "                                    sep=r\"\\s*\", names=['sequence number', 'fortran column','fortran row',\n",
    "                                                       'latitude', 'longitude', 'depth'])\n",
    "                \n",
    "            # Add latitude and longitude\n",
    "            multiple = int(dfLake.shape[0] / mapFile.shape[0])\n",
    "\n",
    "            dfLake['latitude'] =  pd.concat([mapFile] * multiple, ignore_index=True)['latitude'] \n",
    "            dfLake['longitude'] =  pd.concat([mapFile] * multiple, ignore_index=True)['longitude']\n",
    "\n",
    "            # Append lake DataFrames\n",
    "            self.df_ncast = self.df_ncast.append(dfLake, ignore_index=True)\n",
    "            \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        # FCAST \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "        # Loop through lakes\n",
    "        for lake in self.newest_files_fcast.lake.unique():\n",
    "            \n",
    "            # Get DataFrame of attribute files to downhole\n",
    "            df_lake = self.newest_files_fcast[(self.newest_files_fcast.lake == lake)]\n",
    "\n",
    "            # Set attribute counter\n",
    "            attributeCount = 0\n",
    "\n",
    "            # Loop through atributes\n",
    "            for df_index in df_lake.index:\n",
    "\n",
    "                # Update attribute counter\n",
    "                attributeCount += 1\n",
    "\n",
    "                # Get list of attributes based on filetype\n",
    "                cols = self.getAttributeList(self.newest_files_fcast.filetype[df_index], self.attributes)\n",
    "\n",
    "                # Download lake attribute text file and save as DataFrame\n",
    "                dfFile = pd.read_table(\n",
    "                    self.newest_files_fcast.file_url[df_index] + self.newest_files_fcast.filename[df_index], \n",
    "                    header=None,\n",
    "                    names=['data']\n",
    "                )\n",
    "\n",
    "                # Set global variable text file header\n",
    "                global headerString\n",
    "                headerString = dfFile.ix[0, 'data']\n",
    "\n",
    "                # Get text file header as new DataFrame column   \n",
    "                dfFile['header'] = dfFile['data'].map(lambda x: self.getHeaderValue(x))\n",
    "\n",
    "                # Add Lake column\n",
    "                dfFile['lake'] = lake\n",
    "                \n",
    "                # Add CAST Type\n",
    "                dfFile['forecast_type'] = 'FCAST'\n",
    "\n",
    "                # Remove header rows\n",
    "                dfFile['data'] = dfFile['data'].map(lambda x: np.nan if 'dat' in x else x)\n",
    "                dfFile = dfFile.dropna()\n",
    "\n",
    "                # Extract date and map information from header and set as DataFrame columns\n",
    "                dfFile[['year', 'day', 'hour', 'map']] = dfFile['header'].str.split(return_type='frame').ix[:, 0:3]\n",
    "\n",
    "                # Parse attribute column as set as DataFrame columns\n",
    "                dfFile[cols] = dfFile['data'].str.split(return_type='frame')\n",
    "\n",
    "                # Formate map string column\n",
    "                dfFile['map'] = dfFile['map'].map(lambda x: self.getMapFile(x, lake))\n",
    "\n",
    "                # Add Datetime Object\n",
    "                dfFile['datetime'] = dfFile.apply(lambda x: \n",
    "                                                  datetime.strptime(x['year'] + x['day'] + x['hour'], \"%Y%j%H\"), \n",
    "                                                  axis=1)\n",
    "\n",
    "                # Drop useless columns \n",
    "                dfFile = dfFile.drop('data', axis=1).drop('header', axis=1)\n",
    "\n",
    "                # Merge lake specific attribute DataFrames\n",
    "                if attributeCount == 1:\n",
    "                    dfLake = dfFile\n",
    "                else:\n",
    "                    dfLake = pd.merge(dfLake, dfFile[cols + ['datetime']])\n",
    "                \n",
    "            # Get map grid locations\n",
    "            mapFile = pd.read_table(self.url_map + dfFile.ix[dfFile.index[0], 'map'], header=None, \n",
    "                                    sep=r\"\\s*\", names=['sequence number', 'fortran column','fortran row',\n",
    "                                                       'latitude', 'longitude', 'depth'])\n",
    "            \n",
    "            # Add latitude and longitude\n",
    "            multiple = int(dfLake.shape[0] / mapFile.shape[0])\n",
    "\n",
    "            dfLake['latitude'] =  pd.concat([mapFile] * multiple, ignore_index=True)['latitude'] \n",
    "            dfLake['longitude'] =  pd.concat([mapFile] * multiple, ignore_index=True)['longitude']\n",
    "            \n",
    "            # Append lake DataFrames\n",
    "            self.df_fcast = self.df_fcast.append(dfLake, ignore_index=True)\n",
    "            \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        # Merge NCAST and FCAST \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "        # Add NCAST\n",
    "        self.df = self.df_ncast \n",
    "        \n",
    "        # Add FCAST where time > maximum NCAST time\n",
    "        self.df = self.df.append(self.df_fcast[self.df_fcast['datetime'] > self.df_ncast['datetime'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all newest files\n",
    "noaaData = NoaaData(noaa_files, url_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>filetype</th>\n",
       "      <th>lake</th>\n",
       "      <th>file_datetime</th>\n",
       "      <th>current_datetime</th>\n",
       "      <th>forecast_type</th>\n",
       "      <th>file_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e201615912.0.cur</td>\n",
       "      <td>cur</td>\n",
       "      <td>SURFACE CURRENTS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e201615912.0.swt</td>\n",
       "      <td>swt</td>\n",
       "      <td>SURFACE TEMPS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e201615912.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e201615912.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h201615912.0.cur</td>\n",
       "      <td>cur</td>\n",
       "      <td>SURFACE CURRENTS</td>\n",
       "      <td>huron</td>\n",
       "      <td>2016-06-07 12:00:00</td>\n",
       "      <td>2016-06-08 00:54:35.509431</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename file_extension          filetype   lake  \\\n",
       "0  e201615912.0.cur            cur  SURFACE CURRENTS   erie   \n",
       "1  e201615912.0.swt            swt     SURFACE TEMPS   erie   \n",
       "2  e201615912.0.wav            wav             WAVES   erie   \n",
       "3  e201615912.0.wnd            wnd             WINDS   erie   \n",
       "4  h201615912.0.cur            cur  SURFACE CURRENTS  huron   \n",
       "\n",
       "        file_datetime           current_datetime forecast_type  \\\n",
       "0 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "1 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "2 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "3 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "4 2016-06-07 12:00:00 2016-06-08 00:54:35.509431         FCAST   \n",
       "\n",
       "                                            file_url  \n",
       "0  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "2  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "3  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "4  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show newest FCAST files\n",
    "noaaData.newest_files_fcast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoaaData' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fcd2d69480bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnoaaData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'newest_files_fcast'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoaaData' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "noaaData['newest_files_fcast']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 3</h1>\n",
    "<h3>Save Data From Newest Files As DataFrame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>datetime</th>\n",
       "      <th>grid_number</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>map</th>\n",
       "      <th>lake</th>\n",
       "      <th>wave_height</th>\n",
       "      <th>wave_direction</th>\n",
       "      <th>wave_period</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>surface_temperature</th>\n",
       "      <th>current_speed</th>\n",
       "      <th>current_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year, day, hour, datetime, grid_number, latitude, longitude, map, lake, wave_height, wave_direction, wave_period, wind_speed, wind_direction, surface_temperature, current_speed, current_direction]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get newest files\n",
    "noaaData.df_setup()\n",
    "\n",
    "# View DataFrame\n",
    "noaaData.df_ncast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-54e730076269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnoaaData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-234b498aa559>\u001b[0m in \u001b[0;36mdf_fill\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewest_files_fcast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_url\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewest_files_fcast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m                     \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m                     \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m                 )\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    299\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[0;32m    300\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         compression=kwds.get('compression', None))\n\u001b[0m\u001b[0;32m    302\u001b[0m     kwds['compression'] = (inferred_compression if compression == 'infer'\n\u001b[0;32m    303\u001b[0m                            else compression)\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;31m# cat on the compression to the tuple returned by the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         to_return = (list(maybe_read_encoded_stream(req, encoding,\n\u001b[1;32m--> 317\u001b[1;33m                                                     compression)) +\n\u001b[0m\u001b[0;32m    318\u001b[0m                      [compression])\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_return\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mmaybe_read_encoded_stream\u001b[1;34m(reader, encoding, compression)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'gzip'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "noaaData.df_fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View NCAST DataFrame\n",
    "noaaData.df_ncast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View FCAST DataFrame\n",
    "noaaData.df_fcast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View merged DataFrame\n",
    "noaaData.df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV\n",
    "# noaaData.df.to_csv('C:\\\\Users\\\\sgoodfellow\\\\Documents\\\\Sebastian\\\\Projects\\\\Websites\\\\Surfcast\\\\GetData\\\\Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 4</h1>\n",
    "<h3>Load Surf Spots</h3>\n",
    "<h5>Thanks Grif!</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set surf spots file path and file name\n",
    "path = r'C:\\Users\\sgoodfellow\\Documents\\Sebastian\\Projects\\Websites\\Surfcast\\GetData\\SurfSpots'\n",
    "file = 'SurfSpots.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load surf spot data as DataFrame\n",
    "surf_spots = pd.read_csv(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show surf spot data\n",
    "surf_spots.sort('lake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 5</h1>\n",
    "<h3>Plot Raw Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot grid points\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for lake in noaaData.df_ncast.lake.unique():\n",
    "    \n",
    "    df = noaaData.df_ncast[(noaaData.df_ncast.lake == lake) & \n",
    "                               (noaaData.df_ncast.datetime == noaaData.df_ncast.datetime.unique()[0])] \n",
    "    \n",
    "    plt.plot(-df.longitude, df.latitude, marker='.', linestyle='none', ms=2)\n",
    "\n",
    "plt.plot(surf_spots.longitude, surf_spots.latitude, color='k', marker='o', linestyle='none', ms=5)    \n",
    "    \n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=20)\n",
    "plt.ylabel('Latitude', fontsize=20)\n",
    "\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot grid points as 3D scatter\n",
    "attribute = 'wave_height'\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "    \n",
    "df = noaaData.df_ncast[(noaaData.df_ncast.datetime == noaaData.df_ncast.datetime.unique()[0])] \n",
    "\n",
    "sc = plt.scatter(-df.longitude, df.latitude, c = df[attribute], edgecolors='none')\n",
    "\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label(attribute.replace('_', ' ').title(), fontsize=20)\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=20)\n",
    "plt.ylabel('Latitude', fontsize=20)\n",
    "\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 6</h1>\n",
    "<h3>Create SQL Database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializes database with filename 311_8M.db in current directory\n",
    "#surfcast_sql_db = create_engine(r'sqlite:///C:\\Users\\Sebastian\\Projects\\Websites\\Surfcast\\GetData\\Surfcast.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 7</h1>\n",
    "<h3>Update SQL Database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noaa_files.df.to_sql('data', surfcast_sql_db, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql_query('SELECT lake FROM data', surfcast_sql_db)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data  = pd.DataFrame(columns=['year', 'day', 'hour', 'datetime', 'grid_number', 'latitude', 'longitude', 'map', 'lake',\n",
    "                              'grid_number', 'wave_height', 'wave_direction', 'wave_period',\n",
    "                              'wind_speed', 'wind_direction',\n",
    "                              'surface_temperature',\n",
    "                              'currect_speed', 'current_direction',\n",
    "                              'ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOAA attributes\n",
    "wave =        ['grid_number', 'wave_height', 'wave_direction', 'wave_period']\n",
    "wind =        ['grid_number', 'wind_speed', 'wind_direction']\n",
    "temperature = ['grid_number', 'surface_temperature']\n",
    "current =     ['grid_number', 'currect_speed', 'current_direction']\n",
    "ice =         ['grid_number', 'ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']\n",
    "\n",
    "lake = 'superisor'\n",
    "\n",
    "# Load text load\n",
    "df = pd.read_table('http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridded_fields/FCAST/h201615012.0.wav', header=None)\n",
    "\n",
    "# Name text column\n",
    "df.columns = ['data']\n",
    "\n",
    "# Set global variable text file header\n",
    "headerString = df.ix[0, 'data']\n",
    "\n",
    "# Define function to get text file header column\n",
    "def getHeaderValue(val):\n",
    "    \n",
    "    global headerString\n",
    "    \n",
    "    if 'dat' in val and val != headerString:\n",
    "        headerString = val\n",
    "        return headerString\n",
    "    else:\n",
    "        return headerString\n",
    "\n",
    "# Get text file header column    \n",
    "df['header'] = df['data'].map(lambda x: getHeaderValue(x))\n",
    "\n",
    "# Add Lake column\n",
    "df['lake'] = lake\n",
    "\n",
    "# Remove header row\n",
    "df['data'] = df['data'].map(lambda x: np.nan if 'dat' in x else x)\n",
    "df = df.dropna()\n",
    "\n",
    "df['header'].str.split(return_type='frame')\n",
    "\n",
    "# Get Year column\n",
    "df[['year', 'day', 'hour', 'map', 'type', 'point']] = df['header'].str.split(return_type='frame')\n",
    "\n",
    "# Get wave data\n",
    "df[['grid_number', 'wave_height', 'wave_direction', 'wave_period']] = df['data'].str.split(return_type='frame')\n",
    "\n",
    "# Define function to format file\n",
    "def getMapFile(val, lake):\n",
    "    \n",
    "    map = val.split('/')[-1]\n",
    "    map = map.split('.')[0] + '.' + 'map'\n",
    "    \n",
    "    if lake == 'superior':\n",
    "        map = 'superior' + map.split('sup')[1]\n",
    "        \n",
    "    return map\n",
    "\n",
    "# format file\n",
    "df['map'] = df['map'].map(lambda x: getMapFile(x, lake))\n",
    "\n",
    "# Add Datetime Object\n",
    "df['datetime'] = df.apply(lambda x: datetime.strptime(x['year'] + x['day'] + x['hour'], \"%Y%j%H\"), axis=1)\n",
    "\n",
    "# Drop \n",
    "df = df.drop('data', axis=1).drop('header', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yup = df['header'].str.split(return_type='frame')\n",
    "yup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yup[[4, 5]].apply(lambda x : x[0] + ' ' + x[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yup[[4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat([data, df], join='outer', axis = 1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wave =        ['grid_number', 'wave_height', 'wave_direction', 'wave_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wave + ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dun = {\n",
    "    'wave': ['wave_height', 'wave_direction', 'wave_period'],  \n",
    "    'wind': ['wind_speed', 'wind_direction'],\n",
    "    'temperature': ['surface_temperature'],\n",
    "    'current': ['currect_speed', 'current_direction'],\n",
    "    'ice': ['ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dun['wave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapTest = pd.read_table(url_map + 'erie2km.map', header=None, sep=r\"\\s*\", names=['sequence number',\n",
    "                                                                                 'fortran column',\n",
    "                                                                                 'fortran row',\n",
    "                                                                                 'latitude',\n",
    "                                                                                 'longitude',\n",
    "                                                                                 'depth'])\n",
    "mapTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
