{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Surfcast.ca</h1>\n",
    "<h3>A Goodfellow Analytics Creation</h3>\n",
    "<h5>In partnership with Griffin Global</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Reset Notebook\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil import parser\n",
    "from dateutil import tz\n",
    "import scipy.interpolate\n",
    "import os\n",
    "from sqlalchemy import create_engine # database connection\n",
    "\n",
    "# Matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Hide ipython notebook warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 1</h1>\n",
    "<h3>Get HTML Database as List of Files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File types of interest [wave, wind, surface current, surface temperature]\n",
    "#extension_list = ['wav', 'wnd', 'cur', 'swt', 'ice', 'o', 'e', 's', 'm', 'h']\n",
    "extensionList = ['wav', 'wnd','o', 's']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define NOAA database class\n",
    "class NoaaDB:\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Class: NoaaDB\n",
    "        - This class converts the NCAST|FCAST FTP html database list into a pandas DataFrame\n",
    "        \n",
    "        - The gridded fields filename format is:\n",
    "\n",
    "          LYYYYDDDHH.N.EXT\n",
    "\n",
    "          L    = lake letter (s=Superior, m=Michigan, h=Huron, e=Erie, o=Ontario)\n",
    "          YYYY = year at start of simulation (GMT)\n",
    "          DDD  = Day Of Year at start of simulation (GMT)\n",
    "          HH   = hr at start of simulation (GMT)\n",
    "          N    = Site Number\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Set data URLs\n",
    "    dataURL      = 'http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridded_fields/'\n",
    "    ncastDataURL = dataURL + 'NCAST/'\n",
    "    fcastDataURL = dataURL + 'FCAST/'\n",
    "    mapDataURL   = dataURL + 'map_files/'\n",
    "    \n",
    "    # Set NOAA File attributes\n",
    "    attributes = {\n",
    "        'wave':        ['grid_number', 'wave_height', 'wave_direction', 'wave_period'],  \n",
    "        'wind':        ['grid_number', 'wind_speed', 'wind_direction'],\n",
    "        'temperature': ['grid_number', 'surface_temperature'],\n",
    "        'current':     ['grid_number', 'current_speed', 'current_direction'],\n",
    "        'ice':         ['grid_number', 'ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, extensionList):\n",
    "        \n",
    "        # Set Instance attributes\n",
    "        self.extensionList = extensionList       # List of file type extensions   \n",
    "        \n",
    "        # Current UTC time as GMT\n",
    "        self.currentDatetimeGMT = datetime.utcnow().replace(tzinfo=tz.gettz('GMT'))\n",
    "        \n",
    "        # Database dataframe\n",
    "        self.DB = pd.DataFrame(index=[], columns=['fileName', 'fileExtension', \n",
    "                                                  'fileType', 'lake', 'fileDatetime', \n",
    "                                                  'currentDatetime','forecastType', 'fileURL']) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def getFileType(fileExtension):\n",
    "        \n",
    "        if fileExtension == 'wav':           # Wave\n",
    "            return 'WAVES'\n",
    "        elif fileExtension == 'wnd':         # Wind\n",
    "            return 'WINDS'\n",
    "        elif fileExtension == 'cur':         # Surface Current\n",
    "            return 'SURFACE CURRENTS'\n",
    "        elif fileExtension == 'swt':         # Surface Temperature\n",
    "            return 'SURFACE TEMPS'\n",
    "        elif fileExtension == 'ice':         # Ice Conditions\n",
    "            return 'ICE PARAMS'\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def getLake(fileName):\n",
    "        \n",
    "        if fileName[0] == 'e':       # Lake Erie\n",
    "            return 'erie'\n",
    "        elif fileName[0] == 'h':     # Lake Huron\n",
    "            return 'huron'\n",
    "        elif fileName[0] == 'o':     # Lake Ontario\n",
    "            return 'ontario'\n",
    "        elif fileName[0] == 's':     # Lake Superior\n",
    "            return 'superior'\n",
    "        elif fileName[0] == 'm':     # Lake Michigan\n",
    "            return 'michigan'  \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def getDB(self, DBType):\n",
    "        \n",
    "        \"\"\"\n",
    "        Get NOAA Database \n",
    "        \"\"\"\n",
    "        \n",
    "        # Get DataFrame row count\n",
    "        rows = self.DB.shape[0]-1\n",
    "        \n",
    "        # Set Database URL\n",
    "        if DBType == 'NCAST':\n",
    "            url = NoaaDB.ncastDataURL\n",
    "        elif DBType == 'FCAST':\n",
    "            url = NoaaDB.fcastDataURL\n",
    "        \n",
    "        # Get HTML from database page\n",
    "        html = requests.get(url)\n",
    "        \n",
    "        # Create BeautifulSoup object\n",
    "        htmlObj = BeautifulSoup(html.content)\n",
    "        \n",
    "        # Set database list as dataframe \n",
    "        for link in htmlObj.findAll('a', href=True):\n",
    "            if (\n",
    "                link.contents[0].split('.')[-1] in self.extensionList and \n",
    "                link.contents[0][0] in self.extensionList\n",
    "               ):\n",
    "                \n",
    "                rows += 1  # row count                                                 \n",
    "                \n",
    "                fileName      = link.contents[0]                 # file name\n",
    "                fileExtension = link.contents[0].split('.')[-1]  # file extension\n",
    "                \n",
    "                # file datetime as GMT\n",
    "                fileDatetime = datetime.strptime(link.contents[0].split('.')[0]\n",
    "                                                 [1:len(link.contents[0].split('.')[0])], \"%Y%j%H\") \n",
    "                fileDatetime = fileDatetime.replace(tzinfo=tz.gettz('GMT'))\n",
    "                \n",
    "                # Set file type\n",
    "                fileType = NoaaDB.getFileType(fileExtension)\n",
    "                \n",
    "                # Set great lake\n",
    "                lake = NoaaDB.getLake(fileName)          \n",
    "                \n",
    "                # save to dataframe\n",
    "                self.DB.loc[rows] = [fileName, fileExtension, fileType, \n",
    "                                     lake, fileDatetime, self.currentDatetimeGMT, \n",
    "                                     DBType, url]   \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    def getNewestEntries(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Get Newest NCAST and FCAST Files in database and save as DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # NCAST most recent upload time \n",
    "        maxtime = self.DB[(self.DB.forecastType == 'NCAST')]['fileDatetime'].max()  \n",
    "\n",
    "        # Filter DB by NCAST and maxtime\n",
    "        self.newestEntries = self.DB[(self.DB.forecastType == 'NCAST') &\n",
    "                                     (self.DB.fileDatetime == maxtime)].reset_index(drop=True)\n",
    "\n",
    "        # FCAST most recent upload time \n",
    "        maxtime = self.DB[(self.DB.forecastType == 'FCAST')]['fileDatetime'].max()    \n",
    "\n",
    "        # Filter DB by FCAST and maxtime\n",
    "        self.newestEntries = self.newestEntries.append(self.DB[(self.DB.forecastType == 'FCAST') & \n",
    "                                                               (self.DB.fileDatetime == maxtime)]).reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def getHeaderValue(dataString):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to get and update text file header column\n",
    "        \"\"\"\n",
    "\n",
    "        global headerString\n",
    "\n",
    "        if 'dat' in dataString and dataString != headerString:\n",
    "            headerString = dataString\n",
    "            return headerString\n",
    "        else:\n",
    "            return headerString\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def getMapFile(mapString, lake):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to format map string\n",
    "        \"\"\"\n",
    "\n",
    "        map = mapString.split('/')[-1]\n",
    "        map = map.split('.')[0] + '.' + 'map'\n",
    "\n",
    "        if lake == 'superior':\n",
    "            map = 'superior' + map.split('sup')[1]\n",
    "\n",
    "        return map\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def getAttributeList(fileType):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to get list of attributes based on filetype\n",
    "        \"\"\"\n",
    "\n",
    "        if  fileType == 'WAVES':\n",
    "            return NoaaDB.attributes['wave']\n",
    "        elif fileType == 'WINDS':\n",
    "            return NoaaDB.attributes['wind']\n",
    "        elif fileType == 'SURFACE TEMPS':\n",
    "            return NoaaDB.attributes['temperature']\n",
    "        elif fileType == 'SURFACE CURRENTS':\n",
    "            return NoaaDB.attributes['current']\n",
    "        elif fileType == 'ICE PARAMS':\n",
    "            return NoaaDB.attributes['ice']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def setupGriddedFieldsData(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Setup Gridded Fields Data DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get list of unique file types\n",
    "        uniqueFileTypes = self.newestEntries.fileType.unique()\n",
    "        \n",
    "        # Setup Gridded Fields Data DataFrame\n",
    "        griddedFieldsData = pd.DataFrame(columns=['year', 'day', 'hour', 'datetime',                  \n",
    "                                                   'grid_number', 'latitude', 'longitude', \n",
    "                                                   'map', 'lake'])   \n",
    "        \n",
    "        if any('WAVES' in s for s in uniqueFileTypes):                             # wave\n",
    "            for col in NoaaDB.attributes['wave']:\n",
    "                griddedFieldsData[col] = pd.Series(index=griddedFieldsData.index) \n",
    "                \n",
    "        if any('WINDS' in s for s in uniqueFileTypes):                             # wind              \n",
    "            for col in NoaaDB.attributes['wind']:\n",
    "                griddedFieldsData[col] = pd.Series(index=griddedFieldsData.index)\n",
    "                \n",
    "        if any('SURFACE TEMPS' in s for s in uniqueFileTypes):                     # temperature\n",
    "            for col in NoaaDB.attributes['temperature']:\n",
    "                griddedFieldsData[col] = pd.Series(index=griddedFieldsData.index)\n",
    "                \n",
    "        if any('SURFACE CURRENTS' in s for s in uniqueFileTypes):                  # current\n",
    "            for col in NoaaDB.attributes['current']:\n",
    "                griddedFieldsData[col] = pd.Series(index=griddedFieldsData.index)\n",
    "                \n",
    "        if any('ICE PARAMS' in s for s in uniqueFileTypes):                        # ice\n",
    "            for col in NoaaDB.attributes['ice']:\n",
    "                griddedFieldsData[col] = pd.Series(index=griddedFieldsData.index)\n",
    "                \n",
    "        return griddedFieldsData\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def getTextFileData(row):\n",
    "\n",
    "        # Get list of attributes based on filetype\n",
    "        print(row.ix[0, 'fileType'])\n",
    "        cols = NoaaDB.getAttributeList(row.ix[0, 'fileType'])\n",
    "\n",
    "        # Download lake attribute text file and save as DataFrame\n",
    "        dfFile = pd.read_table(\n",
    "            row.ix[0, 'fileURL'] + row.ix[0, 'fileName'], \n",
    "            header=None,\n",
    "            names=['data']\n",
    "        )\n",
    "\n",
    "        # Set global variable text file header\n",
    "        global headerString\n",
    "        headerString = dfFile.ix[0, 'data']\n",
    "\n",
    "        # Get text file header as new DataFrame column   \n",
    "        dfFile['header'] = dfFile['data'].map(lambda x: NoaaDB.getHeaderValue(x))\n",
    "\n",
    "        # Add Lake column\n",
    "        dfFile['lake'] = row.ix[0, 'lake']\n",
    "\n",
    "        # Add CAST Type\n",
    "        dfFile['forecast_type'] = row.ix[0, 'forecastType']\n",
    "\n",
    "        # Remove header rows\n",
    "        dfFile['data'] = dfFile['data'].map(lambda x: np.nan if 'dat' in x else x)\n",
    "        dfFile = dfFile.dropna()\n",
    "\n",
    "        # Extract date and map information from header and set as DataFrame columns\n",
    "        dfFile[['year', 'day', 'hour', 'map']] = dfFile['header'].str.split(return_type='frame').ix[:, 0:3]\n",
    "\n",
    "        # Parse attribute column as set as DataFrame columns\n",
    "        dfFile[cols] = dfFile['data'].str.split(return_type='frame')\n",
    "\n",
    "        # Formate map string column\n",
    "        dfFile['map'] = dfFile['map'].map(lambda x: NoaaDB.getMapFile(x, row.ix[0, 'lake']))\n",
    "\n",
    "        # Add Datetime Object\n",
    "        dfFile['datetime'] = dfFile.apply(lambda x: \n",
    "                                          datetime.strptime(x['year'] + x['day'] + x['hour'], \"%Y%j%H\"), \n",
    "                                          axis=1)\n",
    "\n",
    "        # Drop useless columns \n",
    "        dfFile = dfFile.drop('data', axis=1).drop('header', axis=1)\n",
    "        \n",
    "        # Return text file DataFrame\n",
    "        return dfFile\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    def getAllGriddedFieldsData(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Get gridded fields data\n",
    "        \"\"\"\n",
    "   \n",
    "        # Setup Gridded Fields Data DataFrame\n",
    "        griddedFieldsData = self.setupGriddedFieldsData()\n",
    "                \n",
    "        # Loop through lakes\n",
    "        for lake in self.newestEntries.lake.unique():\n",
    "            \n",
    "            # Set attribute counter\n",
    "            attributeCount = 0\n",
    "            \n",
    "            # Loop through atributes\n",
    "            for fileType in self.newestEntries.fileType.unique():\n",
    "                \n",
    "                # Update attribute counter\n",
    "                attributeCount += 1\n",
    "                \n",
    "                # Get NCAST file data\n",
    "                dfFileNCAST = NoaaDB.getTextFileData(self.newestEntries[\n",
    "                        (self.newestEntries['lake'] == lake) &\n",
    "                        (self.newestEntries['fileType'] == fileType) &\n",
    "                        (self.newestEntries['forecastType'] == 'NCAST')]).reset_index(drop=True)\n",
    "                \n",
    "                # Get FCAST file data\n",
    "                dfFileFCAST = NoaaDB.getTextFileData(self.newestEntries[\n",
    "                        (self.newestEntries['lake'] == lake) &\n",
    "                        (self.newestEntries['fileType'] == fileType) &\n",
    "                        (self.newestEntries['forecastType'] == 'FCAST')]).reset_index(drop=True)\n",
    "                \n",
    "                # Concatenate NCAST and FCAST DataFrames\n",
    "                dfFile = dfFileNCAST\n",
    "                dfFile = dfFile.append(dfFileFCAST[dfFileFCAST['datetime'] > dfFileNCAST['datetime'].max()])\n",
    "\n",
    "                # Merge lake specific attribute DataFrames\n",
    "                if attributeCount == 1:\n",
    "                    dfLake = dfFile\n",
    "                else:\n",
    "                    dfLake = pd.merge(dfLake, dfFile[cols + ['datetime']])\n",
    "\n",
    "            # Append lake DataFrames\n",
    "            griddedFieldsData = griddedFieldsData.append(dfLake, ignore_index=True)\n",
    "\n",
    "        return griddedFieldsData\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create NoaaDB object\n",
    "noaaDB = NoaaDB(extensionList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get NCAST and FCAST Database\n",
    "noaaDB.getDB('NCAST') \n",
    "noaaDB.getDB('FCAST') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>fileExtension</th>\n",
       "      <th>fileType</th>\n",
       "      <th>lake</th>\n",
       "      <th>fileDatetime</th>\n",
       "      <th>currentDatetime</th>\n",
       "      <th>forecastType</th>\n",
       "      <th>fileURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o201615812.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-06 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o201615812.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-06 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o201615818.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-06 18:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o201615818.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-06 18:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o201615900.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-07 00:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fileName fileExtension fileType     lake        fileDatetime  \\\n",
       "0  o201615812.0.wav           wav    WAVES  ontario 2016-06-06 12:00:00   \n",
       "1  o201615812.0.wnd           wnd    WINDS  ontario 2016-06-06 12:00:00   \n",
       "2  o201615818.0.wav           wav    WAVES  ontario 2016-06-06 18:00:00   \n",
       "3  o201615818.0.wnd           wnd    WINDS  ontario 2016-06-06 18:00:00   \n",
       "4  o201615900.0.wav           wav    WAVES  ontario 2016-06-07 00:00:00   \n",
       "\n",
       "             currentDatetime forecastType  \\\n",
       "0 2016-06-14 23:59:44.132585        NCAST   \n",
       "1 2016-06-14 23:59:44.132585        NCAST   \n",
       "2 2016-06-14 23:59:44.132585        NCAST   \n",
       "3 2016-06-14 23:59:44.132585        NCAST   \n",
       "4 2016-06-14 23:59:44.132585        NCAST   \n",
       "\n",
       "                                             fileURL  \n",
       "0  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "2  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "3  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "4  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Dataframe of NoaaDB files \n",
    "noaaDB.DB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>fileExtension</th>\n",
       "      <th>fileType</th>\n",
       "      <th>lake</th>\n",
       "      <th>fileDatetime</th>\n",
       "      <th>currentDatetime</th>\n",
       "      <th>forecastType</th>\n",
       "      <th>fileURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>s201616512.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-13 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>s201616600.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 00:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>s201616600.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 00:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>s201616612.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>s201616612.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fileName fileExtension fileType      lake        fileDatetime  \\\n",
       "177  s201616512.0.wnd           wnd    WINDS  superior 2016-06-13 12:00:00   \n",
       "178  s201616600.0.wav           wav    WAVES  superior 2016-06-14 00:00:00   \n",
       "179  s201616600.0.wnd           wnd    WINDS  superior 2016-06-14 00:00:00   \n",
       "180  s201616612.0.wav           wav    WAVES  superior 2016-06-14 12:00:00   \n",
       "181  s201616612.0.wnd           wnd    WINDS  superior 2016-06-14 12:00:00   \n",
       "\n",
       "               currentDatetime forecastType  \\\n",
       "177 2016-06-14 23:59:44.132585        FCAST   \n",
       "178 2016-06-14 23:59:44.132585        FCAST   \n",
       "179 2016-06-14 23:59:44.132585        FCAST   \n",
       "180 2016-06-14 23:59:44.132585        FCAST   \n",
       "181 2016-06-14 23:59:44.132585        FCAST   \n",
       "\n",
       "                                               fileURL  \n",
       "177  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "178  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "179  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "180  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "181  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Dataframe of NoaaDB files \n",
    "noaaDB.DB.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get most recently uploaded data\n",
    "noaaDB.getNewestEntries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>fileExtension</th>\n",
       "      <th>fileType</th>\n",
       "      <th>lake</th>\n",
       "      <th>fileDatetime</th>\n",
       "      <th>currentDatetime</th>\n",
       "      <th>forecastType</th>\n",
       "      <th>fileURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o201616612.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o201616612.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s201616612.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s201616612.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o201616612.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fileName fileExtension fileType      lake        fileDatetime  \\\n",
       "0  o201616612.0.wav           wav    WAVES   ontario 2016-06-14 12:00:00   \n",
       "1  o201616612.0.wnd           wnd    WINDS   ontario 2016-06-14 12:00:00   \n",
       "2  s201616612.0.wav           wav    WAVES  superior 2016-06-14 12:00:00   \n",
       "3  s201616612.0.wnd           wnd    WINDS  superior 2016-06-14 12:00:00   \n",
       "4  o201616612.0.wav           wav    WAVES   ontario 2016-06-14 12:00:00   \n",
       "\n",
       "             currentDatetime forecastType  \\\n",
       "0 2016-06-14 23:59:44.132585        NCAST   \n",
       "1 2016-06-14 23:59:44.132585        NCAST   \n",
       "2 2016-06-14 23:59:44.132585        NCAST   \n",
       "3 2016-06-14 23:59:44.132585        NCAST   \n",
       "4 2016-06-14 23:59:44.132585        FCAST   \n",
       "\n",
       "                                             fileURL  \n",
       "0  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "2  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "3  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "4  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Dataframe of newest NoaaDB files \n",
    "noaaDB.newestEntries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>fileExtension</th>\n",
       "      <th>fileType</th>\n",
       "      <th>lake</th>\n",
       "      <th>fileDatetime</th>\n",
       "      <th>currentDatetime</th>\n",
       "      <th>forecastType</th>\n",
       "      <th>fileURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s201616612.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>NCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o201616612.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>o201616612.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>ontario</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s201616612.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s201616612.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>superior</td>\n",
       "      <td>2016-06-14 12:00:00</td>\n",
       "      <td>2016-06-14 23:59:44.132585</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fileName fileExtension fileType      lake        fileDatetime  \\\n",
       "3  s201616612.0.wnd           wnd    WINDS  superior 2016-06-14 12:00:00   \n",
       "4  o201616612.0.wav           wav    WAVES   ontario 2016-06-14 12:00:00   \n",
       "5  o201616612.0.wnd           wnd    WINDS   ontario 2016-06-14 12:00:00   \n",
       "6  s201616612.0.wav           wav    WAVES  superior 2016-06-14 12:00:00   \n",
       "7  s201616612.0.wnd           wnd    WINDS  superior 2016-06-14 12:00:00   \n",
       "\n",
       "             currentDatetime forecastType  \\\n",
       "3 2016-06-14 23:59:44.132585        NCAST   \n",
       "4 2016-06-14 23:59:44.132585        FCAST   \n",
       "5 2016-06-14 23:59:44.132585        FCAST   \n",
       "6 2016-06-14 23:59:44.132585        FCAST   \n",
       "7 2016-06-14 23:59:44.132585        FCAST   \n",
       "\n",
       "                                             fileURL  \n",
       "3  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "4  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "5  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "6  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "7  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Dataframe of newest NoaaDB files \n",
    "noaaDB.newestEntries.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAVES\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1944\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1945\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1946\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4154)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4018)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas\\hashtable.c:6610)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas\\hashtable.c:6554)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9e8498e1e5fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get Gridded Fields Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgriddedFieldsData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnoaaDB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAllGriddedFieldsData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# View DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgriddedFieldsData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ce59726b058a>\u001b[0m in \u001b[0;36mgetAllGriddedFieldsData\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m                 dfFileFCAST = NoaaDB.getTextFileData(self.newestEntries[(self.newestEntries['lake'] == lake) &\n\u001b[0;32m    344\u001b[0m                                                                         \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewestEntries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fileType'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mfileType\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                                                                         (self.newestEntries['forecastType'] == 'FCAST')]).reset_index(drop=True)\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m                 \u001b[1;31m# Concatenate NCAST and FCAST DataFrames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ce59726b058a>\u001b[0m in \u001b[0;36mgetTextFileData\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# Get list of attributes based on filetype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fileType'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNoaaDB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAttributeList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fileType'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m                 \u001b[0msection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m                 \u001b[1;31m# we have yielded a scalar ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1016\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no slices here, handle elsewhere'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, copy, drop_level)\u001b[0m\n\u001b[0;32m   1747\u001b[0m                                                       drop_level=drop_level)\n\u001b[0;32m   1748\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1945\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1946\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1947\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4154)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4018)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas\\hashtable.c:6610)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas\\hashtable.c:6554)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Get Gridded Fields Data\n",
    "griddedFieldsData = noaaDB.getAllGriddedFieldsData()\n",
    "\n",
    "# View DataFrame\n",
    "griddedFieldsData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 2</h1>\n",
    "<h3>Find Most Recently Uploaded Files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NoaaData():\n",
    "    \n",
    "    \"\"\"\n",
    "    NOAA database file class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, noaa_files, url_map):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize object\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set object attributes\n",
    "        self.noaa_files = noaa_files              # user input NoaaDB object (Pandas Dataframe of all files in Noaa DB)\n",
    "        self.df_ncast = {}                        # downloaded NCAST text files as DataFrame\n",
    "        self.df_fcast = {}                        # downloaded FCAST text files as DataFrame\n",
    "        self.df = {}                              # conbined NCAST and FCAST DataFrames of most recent 120 hr forecast\n",
    "        self.url_map = url_map                    # url containing map files\n",
    "        \n",
    "        # Set NOAA File attributes\n",
    "        self.attributes = {\n",
    "            'wave':        ['grid_number', 'wave_height', 'wave_direction', 'wave_period'],  \n",
    "            'wind':        ['grid_number', 'wind_speed', 'wind_direction'],\n",
    "            'temperature': ['grid_number', 'surface_temperature'],\n",
    "            'current':     ['grid_number', 'current_speed', 'current_direction'],\n",
    "            'ice':         ['grid_number', 'ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']\n",
    "        }\n",
    "\n",
    "        # Set map path\n",
    "        self.map_path = r'C:\\Users\\Sebastian\\Projects\\Websites\\Surfcast\\GetData\\GridFiles'\n",
    "        \n",
    "        # Get Newest NCAST Files in database and save as DataFrame\n",
    "        maxtime = noaa_files.df[(noaa_files.df.forecast_type == 'NCAST')]['file_datetime'].max()\n",
    "        self.newest_files_ncast = noaa_files.df[(noaa_files.df.forecast_type == 'NCAST') &\n",
    "                                                (noaa_files.df.file_datetime == maxtime)].reset_index(drop=True)\n",
    "        \n",
    "        # Get Newest FCAST Files in database and save as DataFrame\n",
    "        maxtime = noaa_files.df[(noaa_files.df.forecast_type == 'FCAST')]['file_datetime'].max()\n",
    "        self.newest_files_fcast = noaa_files.df[(noaa_files.df.forecast_type == 'FCAST') & \n",
    "                                                (noaa_files.df.file_datetime == maxtime)].reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def df_setup(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Set up empty DataFrames to hold text file data\n",
    "        \"\"\"\n",
    " \n",
    "        # Get list of unique NCAST file types and lakes\n",
    "        filetype_ncast = self.newest_files_ncast.filetype.unique()\n",
    "        \n",
    "        # Setup NCAST DataFrame\n",
    "        self.df_ncast  = pd.DataFrame(columns=['year', 'day', 'hour', 'datetime',                  \n",
    "                                               'grid_number', 'latitude', 'longitude', \n",
    "                                               'map', 'lake'])   \n",
    "        \n",
    "        if any('WAVES' in s for s in filetype_ncast):                         # wave\n",
    "            for col in self.attributes['wave']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index) \n",
    "                \n",
    "        if any('WINDS' in s for s in filetype_ncast):                         # wind              \n",
    "            for col in self.attributes['wind']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index)\n",
    "                \n",
    "        if any('SURFACE TEMPS' in s for s in filetype_ncast):                 # temperature\n",
    "            for col in self.attributes['temperature']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index)\n",
    "                \n",
    "        if any('SURFACE CURRENTS' in s for s in filetype_ncast):              # current\n",
    "            for col in self.attributes['current']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index)\n",
    "                \n",
    "        if any('ICE PARAMS' in s for s in filetype_ncast):                    # ice\n",
    "            for col in self.attributes['ice']:\n",
    "                self.df_ncast[col] = pd.Series(index=self.df_ncast.index)\n",
    "\n",
    "        # Get list of unique FCAST file types and lakes\n",
    "        filetype_fcast = self.newest_files_fcast.filetype.unique()\n",
    "\n",
    "        # Setup FCAST DataFrame\n",
    "        self.df_fcast  = pd.DataFrame(columns=['year', 'day', 'hour', 'datetime',              \n",
    "                                               'grid_number', 'latitude', 'longitude', \n",
    "                                               'map', 'lake'])   \n",
    "        \n",
    "        if any('WAVES' in s for s in filetype_fcast):                         # wave\n",
    "            for col in self.attributes['wave']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index) \n",
    "                \n",
    "        if any('WINDS' in s for s in filetype_fcast):                         # wind              \n",
    "            for col in self.attributes['wind']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index)\n",
    "                \n",
    "        if any('SURFACE TEMPS' in s for s in filetype_fcast):                 # temperature\n",
    "            for col in self.attributes['temperature']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index)\n",
    "                \n",
    "        if any('SURFACE CURRENTS' in s for s in filetype_fcast):              # current\n",
    "            for col in self.attributes['current']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index)\n",
    "                \n",
    "        if any('ICE PARAMS' in s for s in filetype_fcast):                    # ice\n",
    "            for col in self.attributes['ice']:\n",
    "                self.df_fcast[col] = pd.Series(index=self.df_fcast.index)\n",
    "\n",
    "                \n",
    "   \n",
    "\n",
    "    @staticmethod\n",
    "    def getHeaderValue(val):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to get and update text file header column\n",
    "        \"\"\"\n",
    "\n",
    "        global headerString\n",
    "\n",
    "        if 'dat' in val and val != headerString:\n",
    "            headerString = val\n",
    "            return headerString\n",
    "        else:\n",
    "            return headerString\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def getMapFile(val, lake):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to format map string\n",
    "        \"\"\"\n",
    "\n",
    "        map = val.split('/')[-1]\n",
    "        map = map.split('.')[0] + '.' + 'map'\n",
    "\n",
    "        if lake == 'superior':\n",
    "            map = 'superior' + map.split('sup')[1]\n",
    "\n",
    "        return map\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def getAttributeList(fileType, attributes):\n",
    "        \n",
    "        \"\"\"\n",
    "        Define function to get list of attributes based on filetype\n",
    "        \"\"\"\n",
    "\n",
    "        if  fileType == 'WAVES':\n",
    "            return attributes['wave']\n",
    "        elif fileType == 'WINDS':\n",
    "            return attributes['wind']\n",
    "        elif fileType == 'SURFACE TEMPS':\n",
    "            return attributes['temperature']\n",
    "        elif fileType == 'SURFACE CURRENTS':\n",
    "            return attributes['current']\n",
    "        elif fileType == 'ICE PARAMS':\n",
    "            return attributes['ice']\n",
    "        \n",
    "        \n",
    "                \n",
    "                        \n",
    "    def df_fill(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Fill NCAST and FCAST DataFrames with text file data\n",
    "        \"\"\"\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        # NCAST \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "        # Loop through lakes\n",
    "        for lake in self.newest_files_ncast.lake.unique():\n",
    "\n",
    "            # Get DataFrame of attribute files to downhole\n",
    "            df_lake = self.newest_files_ncast[(self.newest_files_ncast.lake == lake)]\n",
    "            \n",
    "            # Set attribute counter\n",
    "            attributeCount = 0\n",
    "            \n",
    "            # Loop through atributes\n",
    "            for df_index in df_lake.index:\n",
    "                \n",
    "                # Update attribute counter\n",
    "                attributeCount += 1\n",
    "                \n",
    "                # Get list of attributes based on filetype\n",
    "                cols = self.getAttributeList(self.newest_files_ncast.filetype[df_index], self.attributes)\n",
    "\n",
    "                # Download lake attribute text file and save as DataFrame\n",
    "                dfFile = pd.read_table(\n",
    "                    self.newest_files_ncast.file_url[df_index] + self.newest_files_ncast.filename[df_index], \n",
    "                    header=None,\n",
    "                    names=['data']\n",
    "                )\n",
    "\n",
    "                # Set global variable text file header\n",
    "                global headerString\n",
    "                headerString = dfFile.ix[0, 'data']\n",
    "       \n",
    "                # Get text file header as new DataFrame column   \n",
    "                dfFile['header'] = dfFile['data'].map(lambda x: self.getHeaderValue(x))\n",
    "\n",
    "                # Add Lake column\n",
    "                dfFile['lake'] = lake\n",
    "                \n",
    "                # Add CAST Type\n",
    "                dfFile['forecast_type'] = 'NCAST'\n",
    "\n",
    "                # Remove header rows\n",
    "                dfFile['data'] = dfFile['data'].map(lambda x: np.nan if 'dat' in x else x)\n",
    "                dfFile = dfFile.dropna()\n",
    "\n",
    "                # Extract date and map information from header and set as DataFrame columns\n",
    "                dfFile[['year', 'day', 'hour', 'map']] = dfFile['header'].str.split(return_type='frame').ix[:, 0:3]\n",
    "\n",
    "                # Parse attribute column as set as DataFrame columns\n",
    "                dfFile[cols] = dfFile['data'].str.split(return_type='frame')\n",
    "\n",
    "                # Formate map string column\n",
    "                dfFile['map'] = dfFile['map'].map(lambda x: self.getMapFile(x, lake))\n",
    "                \n",
    "                # Add Datetime Object\n",
    "                dfFile['datetime'] = dfFile.apply(lambda x: \n",
    "                                                  datetime.strptime(x['year'] + x['day'] + x['hour'], \"%Y%j%H\"), \n",
    "                                                  axis=1)\n",
    "\n",
    "                # Drop useless columns \n",
    "                dfFile = dfFile.drop('data', axis=1).drop('header', axis=1)\n",
    "                \n",
    "                # Merge lake specific attribute DataFrames\n",
    "                if attributeCount == 1:\n",
    "                    dfLake = dfFile\n",
    "                else:\n",
    "                    dfLake = pd.merge(dfLake, dfFile[cols + ['datetime']])\n",
    "                \n",
    "            # Get map grid locations\n",
    "            mapFile = pd.read_table(self.url_map + dfFile.ix[dfFile.index[0], 'map'], header=None, \n",
    "                                    sep=r\"\\s*\", names=['sequence number', 'fortran column','fortran row',\n",
    "                                                       'latitude', 'longitude', 'depth'])\n",
    "                \n",
    "            # Add latitude and longitude\n",
    "            multiple = int(dfLake.shape[0] / mapFile.shape[0])\n",
    "\n",
    "            dfLake['latitude'] =  pd.concat([mapFile] * multiple, ignore_index=True)['latitude'] \n",
    "            dfLake['longitude'] =  pd.concat([mapFile] * multiple, ignore_index=True)['longitude']\n",
    "\n",
    "            # Append lake DataFrames\n",
    "            self.df_ncast = self.df_ncast.append(dfLake, ignore_index=True)\n",
    "            \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        # FCAST \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "        # Loop through lakes\n",
    "        for lake in self.newest_files_fcast.lake.unique():\n",
    "            \n",
    "            # Get DataFrame of attribute files to downhole\n",
    "            df_lake = self.newest_files_fcast[(self.newest_files_fcast.lake == lake)]\n",
    "\n",
    "            # Set attribute counter\n",
    "            attributeCount = 0\n",
    "\n",
    "            # Loop through atributes\n",
    "            for df_index in df_lake.index:\n",
    "\n",
    "                # Update attribute counter\n",
    "                attributeCount += 1\n",
    "\n",
    "                # Get list of attributes based on filetype\n",
    "                cols = self.getAttributeList(self.newest_files_fcast.filetype[df_index], self.attributes)\n",
    "\n",
    "                # Download lake attribute text file and save as DataFrame\n",
    "                dfFile = pd.read_table(\n",
    "                    self.newest_files_fcast.file_url[df_index] + self.newest_files_fcast.filename[df_index], \n",
    "                    header=None,\n",
    "                    names=['data']\n",
    "                )\n",
    "\n",
    "                # Set global variable text file header\n",
    "                global headerString\n",
    "                headerString = dfFile.ix[0, 'data']\n",
    "\n",
    "                # Get text file header as new DataFrame column   \n",
    "                dfFile['header'] = dfFile['data'].map(lambda x: self.getHeaderValue(x))\n",
    "\n",
    "                # Add Lake column\n",
    "                dfFile['lake'] = lake\n",
    "                \n",
    "                # Add CAST Type\n",
    "                dfFile['forecast_type'] = 'FCAST'\n",
    "\n",
    "                # Remove header rows\n",
    "                dfFile['data'] = dfFile['data'].map(lambda x: np.nan if 'dat' in x else x)\n",
    "                dfFile = dfFile.dropna()\n",
    "\n",
    "                # Extract date and map information from header and set as DataFrame columns\n",
    "                dfFile[['year', 'day', 'hour', 'map']] = dfFile['header'].str.split(return_type='frame').ix[:, 0:3]\n",
    "\n",
    "                # Parse attribute column as set as DataFrame columns\n",
    "                dfFile[cols] = dfFile['data'].str.split(return_type='frame')\n",
    "\n",
    "                # Formate map string column\n",
    "                dfFile['map'] = dfFile['map'].map(lambda x: self.getMapFile(x, lake))\n",
    "\n",
    "                # Add Datetime Object\n",
    "                dfFile['datetime'] = dfFile.apply(lambda x: \n",
    "                                                  datetime.strptime(x['year'] + x['day'] + x['hour'], \"%Y%j%H\"), \n",
    "                                                  axis=1)\n",
    "\n",
    "                # Drop useless columns \n",
    "                dfFile = dfFile.drop('data', axis=1).drop('header', axis=1)\n",
    "\n",
    "                # Merge lake specific attribute DataFrames\n",
    "                if attributeCount == 1:\n",
    "                    dfLake = dfFile\n",
    "                else:\n",
    "                    dfLake = pd.merge(dfLake, dfFile[cols + ['datetime']])\n",
    "                \n",
    "            # Get map grid locations\n",
    "            mapFile = pd.read_table(self.url_map + dfFile.ix[dfFile.index[0], 'map'], header=None, \n",
    "                                    sep=r\"\\s*\", names=['sequence number', 'fortran column','fortran row',\n",
    "                                                       'latitude', 'longitude', 'depth'])\n",
    "            \n",
    "            # Add latitude and longitude\n",
    "            multiple = int(dfLake.shape[0] / mapFile.shape[0])\n",
    "\n",
    "            dfLake['latitude'] =  pd.concat([mapFile] * multiple, ignore_index=True)['latitude'] \n",
    "            dfLake['longitude'] =  pd.concat([mapFile] * multiple, ignore_index=True)['longitude']\n",
    "            \n",
    "            # Append lake DataFrames\n",
    "            self.df_fcast = self.df_fcast.append(dfLake, ignore_index=True)\n",
    "            \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        # Merge NCAST and FCAST \n",
    "        # -------------------------------------------------------------------------------------------------------------------- #\n",
    "        \n",
    "        # Add NCAST\n",
    "        self.df = self.df_ncast \n",
    "        \n",
    "        # Add FCAST where time > maximum NCAST time\n",
    "        self.df = self.df.append(self.df_fcast[self.df_fcast['datetime'] > self.df_ncast['datetime'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all newest files\n",
    "noaaData = NoaaData(noaa_files, url_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>filetype</th>\n",
       "      <th>lake</th>\n",
       "      <th>file_datetime</th>\n",
       "      <th>current_datetime</th>\n",
       "      <th>forecast_type</th>\n",
       "      <th>file_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e201616000.0.cur</td>\n",
       "      <td>cur</td>\n",
       "      <td>SURFACE CURRENTS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-06-08</td>\n",
       "      <td>2016-06-08 01:24:33.446723</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e201616000.0.swt</td>\n",
       "      <td>swt</td>\n",
       "      <td>SURFACE TEMPS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-06-08</td>\n",
       "      <td>2016-06-08 01:24:33.446723</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e201616000.0.wav</td>\n",
       "      <td>wav</td>\n",
       "      <td>WAVES</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-06-08</td>\n",
       "      <td>2016-06-08 01:24:33.446723</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e201616000.0.wnd</td>\n",
       "      <td>wnd</td>\n",
       "      <td>WINDS</td>\n",
       "      <td>erie</td>\n",
       "      <td>2016-06-08</td>\n",
       "      <td>2016-06-08 01:24:33.446723</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h201616000.0.cur</td>\n",
       "      <td>cur</td>\n",
       "      <td>SURFACE CURRENTS</td>\n",
       "      <td>huron</td>\n",
       "      <td>2016-06-08</td>\n",
       "      <td>2016-06-08 01:24:33.446723</td>\n",
       "      <td>FCAST</td>\n",
       "      <td>http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename file_extension          filetype   lake file_datetime  \\\n",
       "0  e201616000.0.cur            cur  SURFACE CURRENTS   erie    2016-06-08   \n",
       "1  e201616000.0.swt            swt     SURFACE TEMPS   erie    2016-06-08   \n",
       "2  e201616000.0.wav            wav             WAVES   erie    2016-06-08   \n",
       "3  e201616000.0.wnd            wnd             WINDS   erie    2016-06-08   \n",
       "4  h201616000.0.cur            cur  SURFACE CURRENTS  huron    2016-06-08   \n",
       "\n",
       "            current_datetime forecast_type  \\\n",
       "0 2016-06-08 01:24:33.446723         FCAST   \n",
       "1 2016-06-08 01:24:33.446723         FCAST   \n",
       "2 2016-06-08 01:24:33.446723         FCAST   \n",
       "3 2016-06-08 01:24:33.446723         FCAST   \n",
       "4 2016-06-08 01:24:33.446723         FCAST   \n",
       "\n",
       "                                            file_url  \n",
       "0  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "1  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "2  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "3  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  \n",
       "4  http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridde...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show newest FCAST files\n",
    "noaaData.newest_files_fcast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 3</h1>\n",
    "<h3>Save Data From Newest Files As DataFrame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>datetime</th>\n",
       "      <th>grid_number</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>map</th>\n",
       "      <th>lake</th>\n",
       "      <th>wave_height</th>\n",
       "      <th>wave_direction</th>\n",
       "      <th>wave_period</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>surface_temperature</th>\n",
       "      <th>current_speed</th>\n",
       "      <th>current_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year, day, hour, datetime, grid_number, latitude, longitude, map, lake, wave_height, wave_direction, wave_period, wind_speed, wind_direction, surface_temperature, current_speed, current_direction]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get newest files\n",
    "noaaData.df_setup()\n",
    "\n",
    "# View DataFrame\n",
    "noaaData.df_ncast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-54e730076269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnoaaData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-601cc0201ce6>\u001b[0m in \u001b[0;36mdf_fill\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    229\u001b[0m                 dfFile['datetime'] = dfFile.apply(lambda x: \n\u001b[0;32m    230\u001b[0m                                                   \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hour'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%Y%j%H\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m                                                   axis=1)\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[1;31m# Drop useless columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   4059\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4060\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4061\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4062\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4063\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4115\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_agg_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4116\u001b[0m                     result = lib.reduce(values, func, axis=axis, dummy=dummy,\n\u001b[1;32m-> 4117\u001b[1;33m                                         labels=labels)\n\u001b[0m\u001b[0;32m   4118\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4119\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\reduce.pyx\u001b[0m in \u001b[0;36mpandas.lib.reduce (pandas\\lib.c:43539)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\src\\reduce.pyx\u001b[0m in \u001b[0;36mpandas.lib.Reducer.get_result (pandas\\lib.c:33736)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-601cc0201ce6>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[1;31m# Add Datetime Object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 dfFile['datetime'] = dfFile.apply(lambda x: \n\u001b[1;32m--> 230\u001b[1;33m                                                   \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hour'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%Y%j%H\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m                                                   axis=1)\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sgoodfellow\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   1978\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1979\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 1980\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   1981\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1982\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "noaaData.df_fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View NCAST DataFrame\n",
    "noaaData.df_ncast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View FCAST DataFrame\n",
    "noaaData.df_fcast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View merged DataFrame\n",
    "noaaData.df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV\n",
    "# noaaData.df.to_csv('C:\\\\Users\\\\sgoodfellow\\\\Documents\\\\Sebastian\\\\Projects\\\\Websites\\\\Surfcast\\\\GetData\\\\Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 4</h1>\n",
    "<h3>Load Surf Spots</h3>\n",
    "<h5>Thanks Grif!</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set surf spots file path and file name\n",
    "path = r'C:\\Users\\sgoodfellow\\Documents\\Sebastian\\Projects\\Websites\\Surfcast\\GetData\\SurfSpots'\n",
    "file = 'SurfSpots.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load surf spot data as DataFrame\n",
    "surf_spots = pd.read_csv(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show surf spot data\n",
    "surf_spots.sort('lake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 5</h1>\n",
    "<h3>Plot Raw Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot grid points\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for lake in noaaData.df_ncast.lake.unique():\n",
    "    \n",
    "    df = noaaData.df_ncast[(noaaData.df_ncast.lake == lake) & \n",
    "                               (noaaData.df_ncast.datetime == noaaData.df_ncast.datetime.unique()[0])] \n",
    "    \n",
    "    plt.plot(-df.longitude, df.latitude, marker='.', linestyle='none', ms=2)\n",
    "\n",
    "plt.plot(surf_spots.longitude, surf_spots.latitude, color='k', marker='o', linestyle='none', ms=5)    \n",
    "    \n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=20)\n",
    "plt.ylabel('Latitude', fontsize=20)\n",
    "\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot grid points as 3D scatter\n",
    "attribute = 'wave_height'\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "    \n",
    "df = noaaData.df_ncast[(noaaData.df_ncast.datetime == noaaData.df_ncast.datetime.unique()[0])] \n",
    "\n",
    "sc = plt.scatter(-df.longitude, df.latitude, c = df[attribute], edgecolors='none')\n",
    "\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label(attribute.replace('_', ' ').title(), fontsize=20)\n",
    "plt.tick_params(labelsize=14)\n",
    "\n",
    "plt.xlabel('Longitude', fontsize=20)\n",
    "plt.ylabel('Latitude', fontsize=20)\n",
    "\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 6</h1>\n",
    "<h3>Create SQL Database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializes database with filename 311_8M.db in current directory\n",
    "#surfcast_sql_db = create_engine(r'sqlite:///C:\\Users\\Sebastian\\Projects\\Websites\\Surfcast\\GetData\\Surfcast.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Step 7</h1>\n",
    "<h3>Update SQL Database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noaa_files.df.to_sql('data', surfcast_sql_db, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql_query('SELECT lake FROM data', surfcast_sql_db)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data  = pd.DataFrame(columns=['year', 'day', 'hour', 'datetime', 'grid_number', 'latitude', 'longitude', 'map', 'lake',\n",
    "                              'grid_number', 'wave_height', 'wave_direction', 'wave_period',\n",
    "                              'wind_speed', 'wind_direction',\n",
    "                              'surface_temperature',\n",
    "                              'currect_speed', 'current_direction',\n",
    "                              'ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']) \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOAA attributes\n",
    "wave =        ['grid_number', 'wave_height', 'wave_direction', 'wave_period']\n",
    "wind =        ['grid_number', 'wind_speed', 'wind_direction']\n",
    "temperature = ['grid_number', 'surface_temperature']\n",
    "current =     ['grid_number', 'currect_speed', 'current_direction']\n",
    "ice =         ['grid_number', 'ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']\n",
    "\n",
    "lake = 'superisor'\n",
    "\n",
    "# Load text load\n",
    "df = pd.read_table('http://www.glerl.noaa.gov/ftp/EMF/glcfs/gridded_fields/FCAST/h201615012.0.wav', header=None)\n",
    "\n",
    "# Name text column\n",
    "df.columns = ['data']\n",
    "\n",
    "# Set global variable text file header\n",
    "headerString = df.ix[0, 'data']\n",
    "\n",
    "# Define function to get text file header column\n",
    "def getHeaderValue(val):\n",
    "    \n",
    "    global headerString\n",
    "    \n",
    "    if 'dat' in val and val != headerString:\n",
    "        headerString = val\n",
    "        return headerString\n",
    "    else:\n",
    "        return headerString\n",
    "\n",
    "# Get text file header column    \n",
    "df['header'] = df['data'].map(lambda x: getHeaderValue(x))\n",
    "\n",
    "# Add Lake column\n",
    "df['lake'] = lake\n",
    "\n",
    "# Remove header row\n",
    "df['data'] = df['data'].map(lambda x: np.nan if 'dat' in x else x)\n",
    "df = df.dropna()\n",
    "\n",
    "df['header'].str.split(return_type='frame')\n",
    "\n",
    "# Get Year column\n",
    "df[['year', 'day', 'hour', 'map', 'type', 'point']] = df['header'].str.split(return_type='frame')\n",
    "\n",
    "# Get wave data\n",
    "df[['grid_number', 'wave_height', 'wave_direction', 'wave_period']] = df['data'].str.split(return_type='frame')\n",
    "\n",
    "# Define function to format file\n",
    "def getMapFile(val, lake):\n",
    "    \n",
    "    map = val.split('/')[-1]\n",
    "    map = map.split('.')[0] + '.' + 'map'\n",
    "    \n",
    "    if lake == 'superior':\n",
    "        map = 'superior' + map.split('sup')[1]\n",
    "        \n",
    "    return map\n",
    "\n",
    "# format file\n",
    "df['map'] = df['map'].map(lambda x: getMapFile(x, lake))\n",
    "\n",
    "# Add Datetime Object\n",
    "df['datetime'] = df.apply(lambda x: datetime.strptime(x['year'] + x['day'] + x['hour'], \"%Y%j%H\"), axis=1)\n",
    "\n",
    "# Drop \n",
    "df = df.drop('data', axis=1).drop('header', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yup = df['header'].str.split(return_type='frame')\n",
    "yup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yup[[4, 5]].apply(lambda x : x[0] + ' ' + x[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yup[[4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat([data, df], join='outer', axis = 1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wave =        ['grid_number', 'wave_height', 'wave_direction', 'wave_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wave + ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dun = {\n",
    "    'wave': ['wave_height', 'wave_direction', 'wave_period'],  \n",
    "    'wind': ['wind_speed', 'wind_direction'],\n",
    "    'temperature': ['surface_temperature'],\n",
    "    'current': ['currect_speed', 'current_direction'],\n",
    "    'ice': ['ice_concentration', 'ice_thickness', 'ice_speed', 'ice_direction']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dun['wave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapTest = pd.read_table(url_map + 'erie2km.map', header=None, sep=r\"\\s*\", names=['sequence number',\n",
    "                                                                                 'fortran column',\n",
    "                                                                                 'fortran row',\n",
    "                                                                                 'latitude',\n",
    "                                                                                 'longitude',\n",
    "                                                                                 'depth'])\n",
    "mapTest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
